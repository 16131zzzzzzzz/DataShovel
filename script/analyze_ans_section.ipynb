{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../ans/ans_section.json\", \"r\") as f:\n",
    "  dt = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ABSTRACT', '1 INTRODUCTION', '2 PROBLEM DEFINITION', '3 FRAMEWORK OF SEARCH', '4 TOP-K STAR MATCHING', '5 TOP-K JOIN & PATTERN DECOMPOSITION', '6 EVALUATION', '7 RELATED WORK', '8 CONCLUSION', 'ACKNOWLEDGMENTS'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content2text(content):\n",
    "  text = \"\"\n",
    "  for c in content:\n",
    "    if c[\"type\"] == \"text\":\n",
    "      text += c[\"content\"]\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plain_text(section):\n",
    "  text = \"\"\n",
    "  for num_str, bbox in section['content']['text'].items():\n",
    "    if bbox['content'] != \"\":\n",
    "      text += bbox['content']\n",
    "      text += \"\\n\"\n",
    "  for key in section:\n",
    "    if key != 'content':\n",
    "      text += key\n",
    "      text += \"\\n\"\n",
    "      text += plain_text(section[key])\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this section, we propose the algorithm SMat for computing the top-k star matches. We first give a framework of SMat in the sub-section 4.1. Next, in the sub-section 4.2, we introduce how to efficiently identify the top-k matching nodes for any node of a star pattern. Finally, in the sub-section 4.3, we illustrate how to accurately complete the missing edges of top-k star matches\n",
      "Algorithm SMat\n",
      "Input: a star pattern ${\\mathcal{L}}q,$ an NSGD G, any content vectors $\\mathbf{C}_{\\mathrm{sq}}$ and $\\mathbf{C}_{\\mathrm{g}},$ any structural vector ${\\mathbf{S}}_{\\mathbb{R}},$ integer $\\boldsymbol{k}$ \n",
      "Output: top-k match set ${\\boldsymbol{R}}.$ \n",
      "\n",
      "QUre iatching SMat\n",
      "Fig.4 shows the framework of SMat, where the inputs of SMat are an integer $k,$ a star pattern sq with content vector Csq, and an NSGD G with content and structural vectors $C_{\\mathrm{g}}$ andS ${\\mathbf{S}}_{\\mathbb{g}}.$ ， Its output is the top-k matches ${\\mathfrak{s q}}(G,k)$ of the star pattern sq over G\n",
      "\n",
      "Notice that Csq $C_{\\mathrm{g}}$ and $\\mathbf{S}_{\\mathbb{g}}$ denote the related vectors for an le in sq and G\n",
      "\n",
      "SqMat includes the following three phases\n",
      "Phase 1. SMat first identifies candidate node matches ${\\mathit{V}}_{p}$ and ${\\mathit{V}}_{l}$ for the pivot node $~{\\mathcal{P}}$ and each leaf node ${\\mathit{l}}_{}^{}$ of sq in G (line 3). For each $v_{p}\\in V_{p}$ and $v_{l}\\in V_{l},$ SMat invokes EJud (given in Section 4.3) to judge whether an edge $(v_{p},v_{l})$ should exist in G (line 4). SMat adds $(v_{p},v_{l})$ to G if EJud returns true （line 5). This step can assure a completed set of star matches of sg in G.\n",
      "\n",
      "Next, SMat selects top-h leaf matches $V_{l}^{h}~=~\\{v_{l}^{h}\\}$ for every leaf node with a descending order by calculating the similarity $\\delta(\\cdot)$ between Csq and $C_{\\mathrm{g}}$ (line G). During the identification, SMat invokes NMat (given in Section 4.2) to obtain the top-h leaf matches where $\\ \\ \\!\\ h$ is an integer pre-determined in the system. After that\n",
      "every pivot match $v_{{\\mathcal{X}}}\\,\\!\\rho$ identifies its leaf matches $V_{l}^{p}$ by scanning its neighbors in ${\\cal G},$ and $v_{p}$ can easily obtain its top-l star match after the scanning (lines 7-8)\n",
      "\n",
      " $\\frac{P h a s e}{\\bigr.}$ 2. Among the top-1 star matches at every Up, SqMat selects the best $\\boldsymbol{k}$ star matches to form a pseudo top-k star matches which are inserted in a priority queue ${\\boldsymbol{P}}.$ Note that, these pseudo top-k star matches contain the potential pivot nodes in the final top $\\boldsymbol{k}$ answers (line 9). For the $\\boldsymbol{k}$ pivots in the pseudo top-k star matches, SqMat sorts the leaf matches $V_{l}^{h}\\cup V_{l}^{p}$ for each pivot (line 10). During the $V_{\\,\\,l}^{\\,p}$ to its position in $V_{l}^{h},$ since the sorting, SqMat adds the node in VP\n",
      " $V_{l}^{h}$ have been sorted. The leaf matches pivoted at $v_{P}$ are nodes in Vf\n",
      "sufficed for SMat to obtain the top-k star matches. Therefore, SMat only keeps top-k leaf matches for $v_{p}$ \n",
      "\n",
      "Phase 3.SqMat maintains the priority queue ${\\boldsymbol{P}}.$ SqMat pops up the best match m from I ${\\boldsymbol{P}},$ and inserts it into an answer set ${\\boldsymbol{R}}$ (line 12) For the pivot node in m, it generates the next best match m’ which is inserted into ${\\mathbf{}}P$ (lines 13-14). Specifically, SqMat retrieves every $v_{\\boldsymbol{p}}$ 's leaf match lists and selects the one with the smallest difference by subtracting the largest score in the list. This phase repeatedly until |R|=k. Finally SqMat outputs ${\\boldsymbol{R}}$ as the answer set (line 15)\n",
      "As mentioned above, the value of $\\ \\ \\!\\ h$ is pre-determined in the system. Intuitively, the larger the h is chosen, the more efficiency will be. If $\\ \\ \\!\\ h$ is large enough, $V_{l}^{h}$ \n",
      "Vh may cover many leaf matches in $v_{p}$ 's neighbors. Therefore, the number of remaining neighbors of $v_{p}$ to be scanned and calculated will be small. In contrast, if $\\boldsymbol{\\mathit{h}}$ is very large, every pivot will take more time to obtain its neighbors in the top-h leaf matches. In the experiments, we will choose a proper $\\boldsymbol{\\mathit{h}}$ to optimize the whole search processing.\n",
      "\n",
      "Figure 5: Procedure of star matching\n",
      " $E x a m p l e\\,A.I.$ We demonstrate how SMat computes top-3 matches for the star pattern ${\\mathcal{G}}$ in Fig.5(a) through the three phases. During S0\n",
      "Phase 1, SMat first identifies the candidate node matches $V_{A}\\;=\\;$  $\\{A_{1},...,A_{n}\\}$ of pivot A and leaf nodes candidates $V_{B}$ and $V_{C}.$ Vc. As- suming $h=4,$ it next selects top-4 leaf matches for every leaf node with a descending order given in Fig.5(b) for $V_{B}^{4}$ and $V_{C}^{4}{}.$ After that every pivot candidate $v_{A}\\in V_{A}$ finds its neighbors in $V_{B}^{4}$ and $V_{C}^{44}$ VT\n",
      "For example, the pivot match $A_{3}$  $V_{A}$ finds its neighbors $B_{1},B_{4},$ A3 in VA\n",
      "C2, $C_{3}$ and $C_{4}$ as shown in Fig.5(d). To this end,every pivot match ${\\boldsymbol{v}}_{A}$ can easily obtain its top-1 star matches\n",
      "\n",
      "Phase 2 starts. Among the top-1 star matches at every UA,SMat selects the best three star matches to form a pseudo top-3 star matches given in Fig.5(c). SMat then sorts the leaf matches for each pivot in a descending list. To form the list, SMat needs to scan the neighbors (denoted by $X_{n b r})$ of pivot matches which are not in $V_{l}^{h}.$ For example, in Fig.5(d), among $A_{1}$ 's neighbors SMat A1\n",
      "finds its neighbor Bnbr = 0.85 is larger than $|B_{3}=1$ 0.80, and thu inserts $B_{n b r}$ to its descending list. Similarly, SMat also computes Cnbr = 0.89 for Az and $B_{n b r}$ = 0.70 for A3. At the final of Phase 2, pivot matches $A_{1},A_{2}$ and $A_{3}$ hold the descending lists for leaf matches as given in Fig.5(d)\n",
      "\n",
      "Fig. 5(e) shows the procedure of Phase 3.SMat first pops from priority queue ${\\boldsymbol{P}}$ the best match A2BiC inserted into R, which is the top-1 match pivoted at A2.SMat then generates the next best match A2B1Cmbr pivoted at Az and adds it into P. Following the same rule SqMat terminates until $\\boldsymbol{P}$ pops up A1B2Cz and |R| = 3. Finally the 上\n",
      "answer is returned as $R=\\{A_{2}B_{1}C_{1},A_{2}B_{1}C_{n b r},A_{1}B_{2}C_{2}\\}.$ 口\n",
      "4.2 Top-h Node Matching\n",
      "As shown in the framework of SMat, NMat identifies the top $\\ \\ \\!\\ h$ node matches by calculating the similarity function $\\delta(\\mathbf{C}_{q},\\mathbf{C}_{g}).$ Usually G is very large with billions of nodes (vectors), and hence it is time-consuming to obtain the exact order of the node matches To conquer the problem, NMat leverages Approximate Nearest Neighbors Search (ANNS) [32] to find the top $\\boldsymbol{h}$ node matches. In this subsection, we first introduce the existing ANNS,and then improve ANNS based on the features of content vectors in the NSGD.\n",
      "\n",
      "(1) Approximate nearest neighbors search\n",
      "We first give the definitions for Nearest Neighbors Search (NNS and ANNS\n",
      "\n",
      "Nearest Neighbors Search (NNS). Given a query vector $\\boldsymbol{\\mathit{I}}$ and a finite set of vectors $\\boldsymbol{\\mathsf{S}}$ in the Euclidean space $E^{d}$ with dimension d, NNS obtains ${\\boldsymbol{q}}^{\\prime}$ s $h.$ nearest neighbors (vectors) R by evaluating $\\delta(x,q),$ wherex∈ R is described as follows\n",
      "\n",
      "Approximate Nearest Neighbors Search(ANNS). Given a query vector $\\boldsymbol{\\mathit{I}}$ and a finite set of vectors S in the Euclidean space $E^{d}$ with dimension $d,$ ANNS builds an index $\\overline{{\\cal T}}$ on S. It then gets a subset C of S by T,and evaluates $\\delta(x,q)$ to obtain the approximate $h.$ nearest neighbors R of the query vector g, where x∈ C\n",
      "\n",
      "Due to its widespread adoption,ANNS has developed for a decade. Recently, with the introduction of the approximate lin ear time algorithm for constructing H-Nearest Neighbor Graph (HNNG)[8], researchers developed graph-based ANNS algorithms by constructing HNNG-like graph indices. Such graph indices have an extraordinary ability to express neighbor relationships, which make graph-based ANNS algorithms only need to visit fewer vec- tors in S to yield more accurate answers [12,24]. So in this work NMat advocates and further optimizes the graph-based ANNS al gorithms for finding query q's top-h node matches.\n",
      "\n",
      "An HNNG $\\begin{array}{r}{I_{G}}\\end{array}$ can be constructed as follows[12|: every vector in $\\boldsymbol{\\mathsf{S}}$ is connected to its $\\boldsymbol{\\mathit{h}}$ nearest vectors to form $I_{G}$ in the Euclidean h\n",
      "space $E^{d}.$ A query process on $\\begin{array}{r}{I_{G}}\\end{array}$ is executed as follows.\n",
      "\n",
      "Figure 6: Pr 0n indexIe\n",
      "Assume that graph in Fig. 6 is a built HNNG $\\,I_{G}.$ For a given query node (vector） $\\boldsymbol{\\mathit{I}}$ in I $I_{G},$ ，NMat first randomly selects a node $~{\\mathcal{P}}$  ${\\boldsymbol{\\rho}}^{\\prime}$ \n",
      "as the entry node, follows its out-edges to reach p's neighbors and chooses one to proceed following a principle that minimizes ${\\boldsymbol{q}}.$ the distance to q. After that a new iteration starts from the chosen node. Colored arrows in Fig 6 show the complete query procedure from the entry node $~{\\mathcal{P}}~~~~~~~{}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$ to the query g's nearest neighbor Ci, which needs 5 iterations and 13 distance calculations\n",
      "\n",
      "Issues of ANNS on $I_{G}.$ A social graph or knowledge graph ${\\boldsymbol{G}}$ has many communities, e.g., users sharing the common sport hobby may form a community. Content vectors of nodes in a community $E^{d}$ [15 of G may also constitute a community in the Euclidean space E 35]. Based on the observation, we draw that nodes in the index $\\begin{array}{r}{I_{G}}\\end{array}$ are unevenly distributed and form communities. These features lead to one serious issue of ANNS on $I_{G}\\colon$ the uneven distribution of nodes in the directions (e.g., north and south) of $E^{d}.$  The uneven distribution in directions may lower the search efficiency over $I_{G}.$ For example, it will take more iterations if $\\boldsymbol{\\mathit{I}}$ first searches a wrong direction with more nodes than other directions\n",
      "\n",
      "To solve the issue, NMat designs an edge deletion strategy to alleviate the problem caused by the uneven distribution\n",
      "(2) Edge deletion\n",
      "\n",
      "NMat performs the edge deletion as follows. (1) For any node $\\boldsymbol{u}$ in $I_{G},$ NMat first sorts u's neighbors {v} in an ascending or der of the distance from u to . Denote by $E_{u}$ the edge set ${\\cal E}_{u}\\,=\\,$ {(u,0)|v is a neighbor of ${\\boldsymbol{u}}$ in $\\displaystyle I_{G}\\backslash$ . With the sorted $E_{u},$ NMat chooses the shortest edge $E_{u}$ [0] and marks it as checked.(2) NMat then judges whether there exists an unchecked edge $E_{u}$ L]j > 0) in $E_{u}$ whose angle formed with v is less than a given α.If such EuLi exists, the longer edge will be deleted. This deletion process is exe- cuted for all the remaining unchecked edges in $E_{u}.$ (3) Denote by $E_{u}^{\\prime}$ the the undeleted edge set in $E_{u}$ after the above process. The next iteration starts with the shortest edge in $E_{u}^{\\prime}.$ NMat finishes al H′\n",
      "the iterations when all non-deleted edges have been checked\n",
      "For example, in Fig 6, node ${\\boldsymbol{\\rho}}^{\\prime}$ s neighbors are unevenly distributed in directions. Assuming the angle formed by Pi，p, and ${\\boldsymbol{\\mathit{P}}}2$ is larger than α, the longer edge (p,Pi) will be removed. As a result, the cost of finding g's nearest neighbor will be reduced to 4 search iterations and 10 distance calculations\n",
      "\n",
      "After the edge deletion process, the connectivity of $\\begin{array}{r}{I_{G}}\\end{array}$ C may be destroyed. To still guarantee the connectivity of $\\displaystyle I_{G},$ NMat does the following:(1) NMat first detects the strongly connected components (SCCs) of $I_{G}$ after the edge deletion process. (2）NMat finds out the node nearest to the geometric center of SCC in the embedding\n",
      "space $E^{d}$ d.NMat then spans a breadth-first-search (BFS) tree from the node.(3) For each leaf node of the BFS tree, NMat finds out its approximate nearest neighbors (ANNs) in other SCCs and add edges between leaf nodes and its ANNs\n",
      "\n",
      "4.3 Edge Judging\n",
      "Recall that SMat includes alearning model EJud for judging whether an ${\\mathit{l}}\\cdot$ labeled edge exists between two given nodes u and v.An approach is to apply the embedding techniques for completing knowledge graphs [3,29,34]. Given the nodes u, v and label ${\\mathit{l}},$ the embedding techniques train a learning model that computes thei structural embeddings as $\\mathrm{S}_{u},\\mathrm{S}_{v}$ and Sy. We can obtain their relation\n",
      "By simply applying Equ.(2), we have two issues\n",
      "(1) S always has a probabilistic error due to the embedding tech niques. Because we join multiple star matches to obtain the answer multiple judged edges may be involved in this joining Therefore we could encounter a cascading error incurred by these edges after the joining\n",
      "\n",
      "(2) Nodes u and v contain rich information represented by their content vectors $\\mathbf{S}_{u}$ and $\\mathbf{S}_{v}.$ If we do not consider them as Equ.(2) the error can be further enlarged\n",
      "\n",
      "To solve the two issues, we design the learning model (Fig. 7 EJud that includes two components: symbolic enhancement and entanglement. The symbolic enhancement solves the issue of the cascading error. The entanglement fuses content and structural vectors into the model to return a more accurate judging\n",
      "\n",
      "Given two nodes u and $\\boldsymbol{\\mathit{U}}$ in G, EJud first applies any embedding method [3,29,34] to obtain initial structural vectors Su and S, EJud then inputs $\\mathbf{S}_{u}$ and S, into the symbolic enhancement and Sy\n",
      "entanglement as introduced as follows\n",
      "\n",
      "1) Symbolic enhancement.\n",
      "The idea of symbolic enhancement is to use more symbolic sig nals (i.e.,existed labels in G) to enhance $\\mathbf{S}_{u}$ and $\\mathbf{S}_{v}.$ \n",
      "\n",
      "Specifically, EJud first sets an one-hot vector pu∈{0,1}1X for node ${\\boldsymbol{u}}$ and an adjacent matrix M ∈{0.1}|VIxiVl for label ${\\mathit{l}}_{}^{}$ as their symbolic representations, where ${\\Lambda_{l}^{(u,v)}}_{z}$ = 1 if $l(u,v)\\in G,$ otherwise ${\\bf M}_{l}^{(u,v)}$ = 0. EJud then yields a multi-hot vector with matrix multiplications (Equ.(3)) to represent the nodes linked with u via label ${\\mathit{l}}.$ \n",
      "\n",
      "where $\\scriptstyle{g(\\cdot)}$ is a normalization function: $g({\\bf x})={\\bf_{x}}/s u m({\\bf x})$ .We refen $\\mathbf{p}_{u}$ as u's symbolic vector, and each element of $\\mathbf{p}_{u}$ could be regarded as the probability of the corresponding node\n",
      "\n",
      "Further, assuming the node set with a non-zero probability in $\\mathbf{p}_{u}$ as ${\\mathsf{S P}}_{u},$ EJud employs an aggregation function (Equ.(4)) to compute $\\mathrm{S}_{u}^{\\mathrm{P}}$ \n",
      "a new vector Sf, as u's symbolic enhanced structural vector with an MLP function\n",
      "\n",
      "ere $\\mathfrak{P}_{u}^{\\dagger}$ is the corresponding probability of node $\\dot{\\boldsymbol{l}}$ in $\\mathbf{p}_{u}.$ Therefore node ${\\boldsymbol{u}},$ the yielded new structural vector $\\mathrm{S}_{u}^{\\mathrm{P}}$ gathered mor\n",
      "ground trues after the symbolic enhancement. Accordingly, EJud also obtains the new structural vector $\\mathbf{S}_{\\nu}^{\\mathsf{P}}$ for node v\n",
      "\n",
      "2) Entangling content and structural vectors\n",
      "The idea of entanglement is to fuse the content vector $\\mathbf{C}_{u}$ (resp- $\\mathbf{C}_{\\nu}\\mathbf{)}$ into $\\mathrm{S}_{u}^{\\mathrm{P}}$ (resp. ${\\bf S}_{\\nu}^{\\mathrm{P}}).$ \n",
      "\n",
      "A simple way for entanglement is to concatenate $\\mathbf{C}_{u}$ and $\\mathbf{S}_{u}^{\\mathrm{P}}.$ Con- sidering that the impacts on $\\mathbf{S}_{l}$ from $\\mathbf{C}_{u}$ and $\\mathbf{C}_{v}$ to be distinct, E]ud first introduces an MLP-based attention mechanism to calculate the relevance between $\\mathbf{C}_{u}$ and Sy given by Equ.(5)\n",
      "\n",
      "where vl, W and U are parameters to be trained, and tanh-) is an active function\n",
      "\n",
      "Accordingly, EJud can obtain the relevance Att,l between $\\mathbf{C}_{v}$ and Sy. With the relevance, EJud entangles $\\mathbf{C}_{u}$ and $\\mathrm{S}_{u}^{\\mathrm{P}}$ by concate nating ${\\mathbf C}_{u}\\cdot\\mathrm{Att}_{u,l}$ with $\\mathrm{S}_{u}^{\\mathrm{P}}$ given by Equ.(6)\n",
      "\n",
      "Eu= Concat(Atty.1 ·Cu,\n",
      "EJud can also obtain $\\mathbf{E}_{\\nu}$ for node v\n",
      "(3) Model training.\n",
      "With $\\operatorname{E}_{u}$ and $\\mathbf{E}_{U}.$ ，EJud finally outputs a predicted labe $\\scriptstyle\\operatorname*{lim}$ for edge $(u,v)$ given by Equ.(7)\n",
      "lout = softmax(MLP(Concat(Eu,E,) (7）\n",
      "If lout is the same as the judged label $l_{e}$ ,EJud returns true ialse otherwise\n",
      "\n",
      "The model could be trained by a simple cross entropy loss func tion, as given by Equ.(8）:\n",
      "\n",
      "where Hy is the one hot vector of label L\n",
      "广*/ sh0W7SfhPC01 Cess for building the modelFlud\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(plain_text(dt[\"4 TOP-K STAR MATCHING\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm SMat\n",
      "Input: a star pattern sq, an NSGD G, any content vectors Csq and\n",
      "Cg, any structural vector Sg, integer k\n",
      "\n",
      "Figure 4: Algorithmic framework of star matching SMat.\n",
      "\n",
      "Fig. 4 shows the framework of SMat, where the inputs of SMat\n",
      "are an integer k, a star pattern sq with content vector Csq, and an\n",
      "NSGD G with content and structural vectors Cg and Sg. Its output\n",
      "is the top-k matches sq(G, k) of the star pattern sq over G.\n",
      "\n",
      "Notice that Csq, Cg and Sg denote the related vectors for any\n",
      "node in sq and G.\n",
      "\n",
      "SqMat includes the following three phases.\n",
      "\n",
      "Phase 1. SMat first identifies candidate node matches Vp and Vl for\n",
      "the pivot node p and each leaf node l of sq in G (line 3). For each\n",
      "vp ∈ Vp and vl ∈ Vl , SMat invokes EJud (given in Section 4.3) to\n",
      "judge whether an edge (vp, vl ) should exist in G (line 4). SMat adds\n",
      "(vp, vl ) to G if EJud returns true (line 5). This step can assure a\n",
      "completed set of star matches of sq in G.\n",
      "\n",
      "Next, SMat selects top-h leaf matches V h\n",
      "l\n",
      "} for every\n",
      "leaf node with a descending order by calculating the similarity\n",
      "δ (·) between Csq and Cg (line 6). During the identification, SMat\n",
      "invokes NMat (given in Section 4.2) to obtain the top-h leaf matches,\n",
      "where h is an integer pre-determined in the system. After that,\n",
      "= {vh\n",
      "l\n",
      "\n",
      "∪ V p\n",
      "l\n",
      "to its position in V h\n",
      "l\n",
      "\n",
      "Figure 5: Procedure of star matching.\n",
      "\n",
      "Example 4.1. We demonstrate how SMat computes top-3 matches\n",
      "for the star pattern sq in Fig.5(a) through the three phases. During\n",
      "Phase 1, SMat first identifies the candidate node matches VA =\n",
      "{A1, ..., An } of pivot A and leaf nodes candidates VB and VC . As-\n",
      "suming h = 4, it next selects top-4 leaf matches for every leaf node\n",
      "with a descending order given in Fig.5(b) for V 4\n",
      "C . After that,\n",
      "every pivot candidate vA ∈ VA finds its neighbors in V 4\n",
      "B and V 4\n",
      "C .\n",
      "For example, the pivot match A3 in VA finds its neighbors B1, B4,\n",
      "C2, C3 and C4 as shown in Fig.5(d). To this end, every pivot match\n",
      "vA can easily obtain its top-1 star matches.\n",
      "B and V 4\n",
      "\n",
      "Phase 2 starts. Among the top-1 star matches at every vA, SMat\n",
      "selects the best three star matches to form a pseudo top-3 star\n",
      "matches given in Fig.5(c). SMat then sorts the leaf matches for\n",
      "each pivot in a descending list. To form the list, SMat needs to\n",
      "scan the neighbors (denoted by Xnbr ) of pivot matches which are\n",
      "not in V h\n",
      ". For example, in Fig.5(d), among A1’s neighbors SMat\n",
      "l\n",
      "= 0.85 is larger than B3 = 0.80, and thus\n",
      "finds its neighbor Bnbr\n",
      "inserts Bnbr to its descending list. Similarly, SMat also computes\n",
      "= 0.70 for A3. At the final of Phase\n",
      "Cnbr\n",
      "2, pivot matches A1, A2 and A3 hold the descending lists for leaf\n",
      "matches as given in Fig.5(d).\n",
      "= 0.89 for A2 and Bnbr\n",
      "\n",
      "Fig. 5(e) shows the procedure of Phase 3. SMat first pops from\n",
      "priority queue P the best match A2B1C1 inserted into R, which is the\n",
      "top-1 match pivoted at A2. SMat then generates the next best match\n",
      "A2B1Cnbr pivoted at A2 and adds it into P. Following the same rule,\n",
      "SqMat terminates until P pops up A1B2C2 and |R| = 3. Finally the\n",
      "□\n",
      "answer is returned as R = {A2B1C1, A2B1Cnbr , A1B2C2}.\n",
      "\n",
      "4.2 Top-h Node Matching\n",
      "4.2 Top-h Node Matching\n",
      "As shown in the framework of SMat, NMat identifies the top-\n",
      "h node matches by calculating the similarity function δ (Cq, Cд).\n",
      "Usually G is very large with billions of nodes (vectors), and hence\n",
      "it is time-consuming to obtain the exact order of the node matches.\n",
      "To conquer the problem, NMat leverages Approximate Nearest\n",
      "Neighbors Search (ANNS) [32] to find the top-h node matches. In\n",
      "this subsection, we first introduce the existing ANNS, and then\n",
      "improve ANNS based on the features of content vectors in the\n",
      "NSGD.\n",
      "\n",
      "(1) Approximate nearest neighbors search.\n",
      "\n",
      "We first give the definitions for Nearest Neighbors Search (NNS)\n",
      "and ANNS.\n",
      "\n",
      "Nearest Neighbors Search (NNS). Given a query vector q and a\n",
      "finite set of vectors S in the Euclidean space Ed with dimension\n",
      "d, NNS obtains q’s h-nearest neighbors (vectors) R by evaluating\n",
      "δ (x, q), where x ∈ R is described as follows:\n",
      "\n",
      "Approximate Nearest Neighbors Search (ANNS). Given a query\n",
      "vector q and a finite set of vectors S in the Euclidean space Ed with\n",
      "dimension d, ANNS builds an index I on S. It then gets a subset C\n",
      "of S by I, and evaluates δ (x, q) to obtain the approximate h-nearest\n",
      "neighbors (cid:101)R of the query vector q, where x ∈ C.\n",
      "\n",
      "Due to its widespread adoption, ANNS has developed for a\n",
      "decade. Recently, with the introduction of the approximate lin-\n",
      "ear time algorithm for constructing H-Nearest Neighbor Graph\n",
      "(HNNG) [8], researchers developed graph-based ANNS algorithms\n",
      "by constructing HNNG-like graph indices. Such graph indices have\n",
      "an extraordinary ability to express neighbor relationships, which\n",
      "make graph-based ANNS algorithms only need to visit fewer vec-\n",
      "tors in S to yield more accurate answers [12, 24]. So in this work,\n",
      "NMat advocates and further optimizes the graph-based ANNS al-\n",
      "gorithms for finding query q’s top-h node matches.\n",
      "\n",
      "An HNNG IG can be constructed as follows [12]: every vector in\n",
      "S is connected to its h nearest vectors to form IG in the Euclidean\n",
      "space Ed . A query process on IG is executed as follows.\n",
      "\n",
      "To solve the issue, NMat designs an edge deletion strategy to\n",
      "alleviate the problem caused by the uneven distribution.\n",
      "(2) Edge deletion.\n",
      "\n",
      "NMat performs the edge deletion as follows. (1) For any node\n",
      "u in IG , NMat first sorts u’s neighbors {v} in an ascending or-\n",
      "der of the distance from u to v. Denote by Eu the edge set Eu =\n",
      "{(u, v)|v is a neighbor of u in IG }. With the sorted Eu , NMat chooses\n",
      "the shortest edge Eu [0] and marks it as checked. (2) NMat then\n",
      "judges whether there exists an unchecked edge Eu [j] (j > 0) in\n",
      "Eu whose angle formed with v is less than a given α. If such Eu [j]\n",
      "exists, the longer edge will be deleted. This deletion process is exe-\n",
      "cuted for all the remaining unchecked edges in Eu . (3) Denote by\n",
      "E ′\n",
      "u the the undeleted edge set in Eu after the above process. The\n",
      "next iteration starts with the shortest edge in E ′\n",
      "u . NMat finishes all\n",
      "the iterations when all non-deleted edges have been checked.\n",
      "\n",
      "For example, in Fig 6, node p’s neighbors are unevenly distributed\n",
      "in directions. Assuming the angle formed by p1, p, and p2 is larger\n",
      "than α, the longer edge (p, p1) will be removed. As a result, the cost\n",
      "of finding q’s nearest neighbor will be reduced to 4 search iterations\n",
      "and 10 distance calculations.\n",
      "\n",
      "After the edge deletion process, the connectivity of IG may be\n",
      "destroyed. To still guarantee the connectivity of IG , NMat does the\n",
      "following: (1) NMat first detects the strongly connected components\n",
      "(SCCs) of IG after the edge deletion process. (2) NMat finds out\n",
      "the node nearest to the geometric center of SCC in the embedding\n",
      "\n",
      "space Ed . NMat then spans a breadth-first-search (BFS) tree from\n",
      "the node. (3) For each leaf node of the BFS tree, NMat finds out\n",
      "its approximate nearest neighbors (ANNs) in other SCCs and add\n",
      "edges between leaf nodes and its ANNs.\n",
      "\n",
      "4.3 Edge Judging\n",
      "(2) Nodes u and v contain rich information represented by their\n",
      "content vectors Su and Sv . If we do not consider them as Equ. (2),\n",
      "the error can be further enlarged.\n",
      "\n",
      "To solve the two issues, we design the learning model (Fig. 7)\n",
      "EJud that includes two components: symbolic enhancement and\n",
      "entanglement. The symbolic enhancement solves the issue of the\n",
      "cascading error. The entanglement fuses content and structural\n",
      "vectors into the model to return a more accurate judging.\n",
      "\n",
      "Given two nodes u and v in G, EJud first applies any embedding\n",
      "method [3, 29, 34] to obtain initial structural vectors Su and Sv .\n",
      "EJud then inputs Su and Sv into the symbolic enhancement and\n",
      "entanglement as introduced as follows.\n",
      "\n",
      "(1) Symbolic enhancement.\n",
      "\n",
      "The idea of symbolic enhancement is to use more symbolic sig-\n",
      "nals (i.e., existed labels in G) to enhance Su and Sv .\n",
      "\n",
      "Specifically, EJud first sets an one-hot vector pu ∈ {0, 1}1×|V |\n",
      "for node u and an adjacent matrix Ml ∈ {0, 1} |V |×|V | for label l\n",
      "(u,v)\n",
      "= 1 if l(u, v) ∈ G,\n",
      "as their symbolic representations, where M\n",
      "l\n",
      "(u,v)\n",
      "otherwise M\n",
      "= 0. EJud then yields a multi-hot vector with\n",
      "l\n",
      "matrix multiplications (Equ. (3)) to represent the nodes linked with\n",
      "u via label l.\n",
      "\n",
      "′\n",
      "Further, assuming the node set with a non-zero probability in p\n",
      "u\n",
      "as SPu , EJud employs an aggregation function (Equ. (4)) to compute\n",
      "a new vector SP\n",
      "u as u’s symbolic enhanced structural vector with\n",
      "an MLP function:\n",
      "\n",
      "where pi′\n",
      "for node u, the yielded new structural vector SP\n",
      "′\n",
      "u is the corresponding probability of node i in p\n",
      "u. Therefore,\n",
      "u gathered more\n",
      "\n",
      "v for node v.\n",
      "\n",
      "The idea of entanglement is to fuse the content vector Cu (resp.\n",
      "Cv) into SP\n",
      "u (resp. SP\n",
      "v ).\n",
      "\n",
      "A simple way for entanglement is to concatenate Cu and SP\n",
      "u. Con-\n",
      "sidering that the impacts on Sl from Cu and Cv to be distinct, EJud\n",
      "first introduces an MLP-based attention mechanism to calculate\n",
      "the relevance between Cu and Sl given by Equ. (5).\n",
      "\n",
      "where VT, W and U are parameters to be trained, and tanh(·) is an\n",
      "active function.\n",
      "\n",
      "Accordingly, EJud can obtain the relevance Attv,l between Cv\n",
      "u by concate-\n",
      "and Sl . With the relevance, EJud entangles Cu and SP\n",
      "nating Cu · Attu,l with SP\n",
      "\n",
      "EJud can also obtain Ev for node v.\n",
      "\n",
      "(3) Model training.\n",
      "With Eu and Ev , EJud finally outputs a predicted label lout for\n",
      "edge (u, v) given by Equ. (7).\n",
      "lout = softmax(MLP(Concat(Eu, Ev)))\n",
      "(7)\n",
      "\n",
      "If lout is the same as the judged label le , EJud returns true and\n",
      "false otherwise.\n",
      "\n",
      "The model could be trained by a simple cross entropy loss func-\n",
      "tion, as given by Equ. (8):\n",
      "\n",
      "where Hl is the one hot vector of label l .\n",
      "\n",
      "Fig. 7 shows the complete process for building the model EJud.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(plain_text(dt[\"4 TOP-K STAR MATCHING\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
